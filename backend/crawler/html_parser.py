"""
HTML Parser - ä»HTMLå¿«ç…§ä¸­æå–startupæ•°æ®
"""

import re
from pathlib import Path
from typing import Dict, Any, Optional, List, Union
from bs4 import BeautifulSoup
from dataclasses import dataclass, asdict
import json


@dataclass
class StartupData:
    """Startupæ•°æ®ç»“æ„"""
    # åŸºæœ¬ä¿¡æ¯
    name: str = ""
    slug: str = ""
    description: str = ""
    website_url: str = ""  # äº§å“å®˜ç½‘é“¾æ¥
    logo_url: str = ""
    trustmrr_url: str = ""  # TrustMRRé¡µé¢é“¾æ¥

    # æ”¶å…¥æ•°æ®
    total_revenue: str = ""
    total_revenue_raw: int = 0
    mrr: str = ""
    mrr_raw: int = 0
    revenue_last_4_weeks: str = ""
    revenue_last_4_weeks_raw: int = 0
    revenue_change_percent: str = ""
    active_subscriptions: int = 0

    # æ’å
    rank: int = 0

    # åˆ›å§‹äººä¿¡æ¯
    founder_name: str = ""
    founder_username: str = ""
    founder_followers: int = 0  # ç²‰ä¸æ•°é‡
    founder_social_platform: str = ""  # ç¤¾äº¤å¹³å° (å¦‚ ğ•, Twitter, LinkedIn)
    founder_profile_url: str = ""
    founder_avatar_url: str = ""  # åˆ›å§‹äººå¤´åƒURL

    # å…¬å¸ä¿¡æ¯
    founded: str = ""
    country: str = ""
    country_code: str = ""
    category: str = ""
    category_slug: str = ""

    # å‡ºå”®ä¿¡æ¯
    is_for_sale: bool = False
    asking_price: str = ""
    asking_price_raw: int = 0
    revenue_multiple: str = ""
    buyers_interested: int = 0  # æœ€è¿‘å…³æ³¨/æŸ¥çœ‹çš„ä¹°å®¶æ•°é‡

    # å…ƒæ•°æ®
    last_updated: str = ""
    verified_source: str = ""

    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


class HTMLParser:
    """ä»HTMLå¿«ç…§è§£æstartupæ•°æ®"""

    def __init__(self, html_content: str):
        self.soup = BeautifulSoup(html_content, 'lxml')
        self.data = StartupData()

    @classmethod
    def from_file(cls, file_path: Union[str, Path]) -> 'HTMLParser':
        """ä»æ–‡ä»¶åŠ è½½"""
        path = Path(file_path)
        content = path.read_text(encoding='utf-8')
        parser = cls(content)
        # ä»æ–‡ä»¶åè·å–slug
        parser.data.slug = path.stem
        return parser

    def parse(self) -> StartupData:
        """è§£ææ‰€æœ‰æ•°æ®"""
        self._parse_basic_info()
        self._parse_revenue_cards()
        self._parse_founder_info()
        self._parse_company_info()
        self._parse_sale_info()
        self._parse_metadata()
        return self.data

    def _parse_basic_info(self) -> None:
        """è§£æåŸºæœ¬ä¿¡æ¯ï¼šåç§°ã€æè¿°ã€ç½‘ç«™URLã€Logo"""
        # åç§° - ä»h1æ ‡ç­¾è·å–
        h1 = self.soup.find('h1', class_=re.compile(r'text-[23]xl.*font-bold'))
        if h1:
            self.data.name = h1.get_text(strip=True)

        # æè¿° - ä»ç‰¹å®šçš„pæ ‡ç­¾è·å–
        desc_p = self.soup.find('p', class_=re.compile(r'text-sm.*text-muted-foreground.*leading-relaxed'))
        if desc_p:
            self.data.description = desc_p.get_text(strip=True)

        # äº§å“å®˜ç½‘URL - ä»VisitæŒ‰é’®è·å–
        visit_link = self.soup.find('a', string=re.compile(r'Visit'))
        if not visit_link:
            # å°è¯•æ‰¾åŒ…å«Visitæ–‡æœ¬çš„aæ ‡ç­¾
            for a in self.soup.find_all('a', target='_blank'):
                if 'Visit' in a.get_text():
                    visit_link = a
                    break

        if visit_link and visit_link.get('href'):
            url = visit_link['href']
            # ç§»é™¤trackingå‚æ•°
            if '?ref=' in url:
                url = url.split('?ref=')[0]
            self.data.website_url = url

        # TrustMRRé¡µé¢é“¾æ¥
        if self.data.slug:
            self.data.trustmrr_url = f"https://trustmrr.com/startup/{self.data.slug}"

        # Logo URL
        if self.data.name:
            logo_img = self.soup.find('img', alt=self.data.name)
            if logo_img and logo_img.get('src'):
                src = logo_img['src']
                # è½¬æ¢ä¸ºå®Œæ•´URL
                if src.startswith('/'):
                    src = f"https://trustmrr.com{src}"
                self.data.logo_url = src

    def _parse_revenue_cards(self) -> None:
        """è§£ææ”¶å…¥ç›¸å…³çš„å¡ç‰‡æ•°æ®"""
        cards = self.soup.find_all('div', class_=re.compile(r'bg-card.*rounded-xl.*border'))

        for card in cards:
            card_text = card.get_text()

            # Total revenue å¡ç‰‡
            if 'Total revenue' in card_text:
                self._parse_total_revenue_card(card)

            # MRR å¡ç‰‡
            elif 'MRR' in card_text:
                self._parse_mrr_card(card)

            # Revenue last 4 weeks å¡ç‰‡ (å›¾è¡¨å¡ç‰‡)
            elif 'revenue last 4 weeks' in card_text:
                self._parse_recent_revenue_card(card)

    def _parse_total_revenue_card(self, card) -> None:
        """è§£ææ€»æ”¶å…¥å¡ç‰‡"""
        # æ”¶å…¥é‡‘é¢
        amount_div = card.find('div', class_=re.compile(r'text-2xl.*font-bold'))
        if amount_div:
            self.data.total_revenue = amount_div.get_text(strip=True)
            self.data.total_revenue_raw = self._parse_money(self.data.total_revenue)

        # æ’å
        rank_span = card.find('span', class_=re.compile(r'cursor-help'))
        if rank_span:
            rank_text = rank_span.get_text(strip=True)
            match = re.search(r'#(\d+)', rank_text)
            if match:
                self.data.rank = int(match.group(1))

        # å˜åŒ–ç™¾åˆ†æ¯” - æŸ¥æ‰¾åŒ…å« "MoM growth" çš„div
        growth_div = card.find('div', title=re.compile(r'vs previous'))
        if growth_div:
            # ä»titleå±æ€§æå–ç™¾åˆ†æ¯”
            title = growth_div.get('title', '')
            match = re.search(r'(\d+(?:\.\d+)?)\s*%', title)
            if match:
                self.data.revenue_change_percent = f"{match.group(1)}%"

        # å¤‡é€‰æ–¹æ¡ˆï¼šä»spanä¸­æŸ¥æ‰¾
        if not self.data.revenue_change_percent:
            growth_span = card.find('span', class_=re.compile(r'text-xs.*font-semibold.*(text-red|text-green)'))
            if growth_span:
                text = growth_span.get_text(strip=True)
                if '%' in text:
                    self.data.revenue_change_percent = text

    def _parse_mrr_card(self, card) -> None:
        """è§£æMRRå¡ç‰‡"""
        # MRRé‡‘é¢
        amount_div = card.find('div', class_=re.compile(r'text-2xl.*font-bold'))
        if amount_div:
            self.data.mrr = amount_div.get_text(strip=True)
            self.data.mrr_raw = self._parse_money(self.data.mrr)

        # Active subscriptions
        sub_p = card.find('p', class_=re.compile(r'text-xs.*text-muted-foreground'))
        if sub_p:
            sub_text = sub_p.get_text(strip=True)
            match = re.search(r'(\d+)\s*active\s*subscription', sub_text)
            if match:
                self.data.active_subscriptions = int(match.group(1))

    def _parse_recent_revenue_card(self, card) -> None:
        """è§£æè¿‘æœŸæ”¶å…¥å¡ç‰‡"""
        # é‡‘é¢
        amount_div = card.find('div', class_=re.compile(r'text-2xl.*font-bold'))
        if amount_div:
            self.data.revenue_last_4_weeks = amount_div.get_text(strip=True)
            self.data.revenue_last_4_weeks_raw = self._parse_money(self.data.revenue_last_4_weeks)

    def _parse_founder_info(self) -> None:
        """è§£æåˆ›å§‹äººä¿¡æ¯"""
        # æ‰¾åˆ°Founderå¡ç‰‡
        founder_link = self.soup.find('a', href=re.compile(r'/founder/'))
        if not founder_link:
            return

        # Founder profile URL
        href = founder_link.get('href', '')
        if href.startswith('/'):
            self.data.founder_profile_url = f"https://trustmrr.com{href}"
        else:
            self.data.founder_profile_url = href

        # Username from URL
        match = re.search(r'/founder/([^/]+)', self.data.founder_profile_url)
        if match:
            self.data.founder_username = match.group(1)

        # Founder avatar - æŸ¥æ‰¾ rounded-full çš„ img æ ‡ç­¾
        avatar_img = founder_link.find('img', class_=re.compile(r'rounded-full'))
        if avatar_img and avatar_img.get('src'):
            self.data.founder_avatar_url = avatar_img['src']
        elif self.data.founder_username:
            # å¦‚æœæ²¡æ‰¾åˆ°å¤´åƒï¼Œä½¿ç”¨ unavatar.io æœåŠ¡æ„å»º
            self.data.founder_avatar_url = f"https://unavatar.io/x/{self.data.founder_username}"

        # Founder name
        name_span = founder_link.find('span', class_=re.compile(r'truncate'))
        if name_span:
            self.data.founder_name = name_span.get_text(strip=True)

        # Followers and platform - æŸ¥æ‰¾ "X followers on Y" æ¨¡å¼
        followers_text = founder_link.get_text()

        # åŒ¹é… "135 followers on ğ•" æˆ– "1.2k followers on Twitter" ç­‰
        followers_match = re.search(r'([\d,.]+[kKmM]?)\s*followers?\s*(?:on\s+)?([ğ•XTwitterLinkedIn]*)?', followers_text, re.IGNORECASE)
        if followers_match:
            followers_str = followers_match.group(1).replace(',', '')
            # å¤„ç† k/m åç¼€
            if followers_str.lower().endswith('k'):
                self.data.founder_followers = int(float(followers_str[:-1]) * 1000)
            elif followers_str.lower().endswith('m'):
                self.data.founder_followers = int(float(followers_str[:-1]) * 1000000)
            else:
                try:
                    self.data.founder_followers = int(float(followers_str))
                except ValueError:
                    self.data.founder_followers = 0

            # ç¤¾äº¤å¹³å°
            platform = followers_match.group(2)
            if platform:
                # ğ• æ˜¯Twitterçš„æ–°æ ‡è¯†
                if platform in ['ğ•', 'X', 'x']:
                    self.data.founder_social_platform = 'X (Twitter)'
                else:
                    self.data.founder_social_platform = platform

    def _parse_company_info(self) -> None:
        """è§£æå…¬å¸ä¿¡æ¯ï¼šæˆç«‹æ—¶é—´ã€å›½å®¶ã€åˆ†ç±»"""
        # æ‰¾åˆ°Foundedå¡ç‰‡
        cards = self.soup.find_all('div', class_=re.compile(r'bg-card.*rounded-xl.*border'))

        for card in cards:
            card_text = card.get_text()
            if 'Founded' not in card_text:
                continue

            # Founded date - æŸ¥æ‰¾åŒ…å«æœˆä»½çš„æ–‡æœ¬
            # HTMLç»“æ„: <div class="text-2xl font-bold"><div class="flex flex-col"><div>December 2023</div>...
            amount_div = card.find('div', class_=re.compile(r'text-2xl.*font-bold'))
            if amount_div:
                # æŸ¥æ‰¾æ‰€æœ‰çº¯æ–‡æœ¬divï¼ˆä¸æ˜¯é“¾æ¥ï¼‰
                date_months = ['January', 'February', 'March', 'April', 'May', 'June',
                               'July', 'August', 'September', 'October', 'November', 'December']

                # éå†æ‰€æœ‰divå­å…ƒç´ 
                for div in amount_div.find_all('div', recursive=True):
                    # è·³è¿‡åŒ…å«é“¾æ¥çš„div
                    if div.find('a'):
                        continue

                    text = div.get_text(strip=True)
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æ—¥æœŸæ ¼å¼ï¼ˆåŒ…å«æœˆä»½å + å¹´ä»½ï¼‰
                    if text and any(month in text for month in date_months):
                        # ç¡®ä¿åªåŒ…å«æ—¥æœŸï¼Œä¸åŒ…å«å›½å®¶å
                        # æ—¥æœŸæ ¼å¼é€šå¸¸æ˜¯ "Month Year" æˆ– "Month YYYY"
                        if re.match(r'^[A-Za-z]+\s+\d{4}$', text):
                            self.data.founded = text
                            break

            # Country - ä»countryé“¾æ¥è·å–
            country_link = card.find('a', href=re.compile(r'/country/'))
            if country_link:
                country_span = country_link.find('span', class_=re.compile(r'text-muted-foreground'))
                if country_span:
                    self.data.country = country_span.get_text(strip=True)
                # Country code from URL
                match = re.search(r'/country/([^/]+)', country_link.get('href', ''))
                if match:
                    self.data.country_code = match.group(1).upper()

            # Category - ä»categoryé“¾æ¥è·å–
            category_link = card.find('a', href=re.compile(r'/category/'))
            if category_link:
                category_span = category_link.find('span')
                if category_span:
                    self.data.category = category_span.get_text(strip=True)
                # Category slug from URL
                match = re.search(r'/category/([^/]+)', category_link.get('href', ''))
                if match:
                    self.data.category_slug = match.group(1)

            break

    def _parse_sale_info(self) -> None:
        """è§£æå‡ºå”®ä¿¡æ¯"""
        # æ£€æŸ¥æ˜¯å¦æœ‰å‡ºå”®æ¨ªå¹…
        sale_banner = self.soup.find(string=re.compile(r'This startup is for sale'))
        if not sale_banner:
            return

        self.data.is_for_sale = True

        # æ‰¾åˆ°åŒ…å«å‡ºå”®ä¿¡æ¯çš„banner
        banner = self.soup.find('div', class_=re.compile(r'bg-gradient-to-r.*amber'))
        if not banner:
            # å°è¯•å…¶ä»–æ–¹å¼æŸ¥æ‰¾
            banner = sale_banner.find_parent('div', class_=re.compile(r'sticky|bg-'))

        if not banner:
            return

        banner_text = banner.get_text()

        # Asking price - æŸ¥æ‰¾ "Asking price: $X" æ¨¡å¼
        price_match = re.search(r'Asking price[:\s]*\$?([\d,]+(?:\.\d+)?[kKmM]?)', banner_text)
        if price_match:
            self.data.asking_price = f"${price_match.group(1)}"
            self.data.asking_price_raw = self._parse_money(price_match.group(1))
        else:
            # å¤‡é€‰ï¼šæŸ¥æ‰¾font-boldçš„span
            price_span = banner.find('span', class_=re.compile(r'font-bold'))
            if price_span:
                price_text = price_span.get_text(strip=True)
                if '$' in price_text or price_text.replace(',', '').replace('.', '').isdigit():
                    self.data.asking_price = price_text
                    self.data.asking_price_raw = self._parse_money(price_text)

        # Revenue multiple - æŸ¥æ‰¾ "X.Xx revenue" æ¨¡å¼
        multiple_match = re.search(r'([\d.]+)x\s*(?:revenue)?', banner_text, re.IGNORECASE)
        if multiple_match:
            self.data.revenue_multiple = f"{multiple_match.group(1)}x"

        # Buyers interested - æŸ¥æ‰¾ "X buyers saw this" æ¨¡å¼
        buyers_match = re.search(r'(\d+)\s*(?:buyers?\s*(?:saw|viewed|interested)|people\s*viewed)', banner_text, re.IGNORECASE)
        if buyers_match:
            self.data.buyers_interested = int(buyers_match.group(1))

    def _parse_metadata(self) -> None:
        """è§£æå…ƒæ•°æ®ï¼šéªŒè¯æ¥æºã€æœ€åæ›´æ–°æ—¶é—´"""
        # éªŒè¯æ¥æº
        verified_text = self.soup.find(string=re.compile(r'verified with'))
        if verified_text:
            if 'Stripe' in verified_text:
                self.data.verified_source = 'Stripe'
            elif 'Paddle' in verified_text:
                self.data.verified_source = 'Paddle'
            else:
                self.data.verified_source = 'Unknown'

        # Last updated
        updated_span = self.soup.find('span', string=re.compile(r'Last updated'))
        if updated_span:
            match = re.search(r'Last updated:\s*(.+)', updated_span.get_text())
            if match:
                self.data.last_updated = match.group(1).strip()

    @staticmethod
    def _parse_money(text: str) -> int:
        """å°†é‡‘é¢å­—ç¬¦ä¸²è§£æä¸ºæ•´æ•°ï¼ˆå•ä½ï¼šç¾åˆ†æˆ–ç¾å…ƒï¼‰"""
        if not text:
            return 0
        # ç§»é™¤$ç¬¦å·å’Œé€—å·
        text = text.replace('$', '').replace(',', '').strip()
        # å¤„ç†kåç¼€
        if text.lower().endswith('k'):
            return int(float(text[:-1]) * 1000)
        # å¤„ç†måç¼€
        if text.lower().endswith('m'):
            return int(float(text[:-1]) * 1000000)
        try:
            return int(float(text))
        except ValueError:
            return 0


def parse_html_file(file_path: Union[str, Path]) -> Dict[str, Any]:
    """ä¾¿æ·å‡½æ•°ï¼šè§£æHTMLæ–‡ä»¶å¹¶è¿”å›å­—å…¸"""
    parser = HTMLParser.from_file(file_path)
    data = parser.parse()
    return data.to_dict()


def parse_all_snapshots(snapshot_dir: Union[str, Path]) -> List[Dict[str, Any]]:
    """è§£æç›®å½•ä¸­æ‰€æœ‰HTMLå¿«ç…§"""
    snapshot_dir = Path(snapshot_dir)
    results = []

    for html_file in snapshot_dir.glob('*.html'):
        try:
            data = parse_html_file(html_file)
            results.append(data)
            print(f"Parsed: {html_file.name} -> {data.get('name', 'Unknown')}")
        except Exception as e:
            print(f"Error parsing {html_file.name}: {e}")

    return results


# CLIå…¥å£
if __name__ == '__main__':
    import sys

    if len(sys.argv) > 1:
        # è§£ææŒ‡å®šæ–‡ä»¶
        file_path = sys.argv[1]
        data = parse_html_file(file_path)
        print(json.dumps(data, indent=2, ensure_ascii=False))
    else:
        # è§£ææ‰€æœ‰å¿«ç…§
        snapshot_dir = Path(__file__).parent.parent / 'data' / 'html_snapshots'
        if snapshot_dir.exists():
            results = parse_all_snapshots(snapshot_dir)
            print(f"\n{'='*60}")
            print(f"Total parsed: {len(results)} files")

            # æ‰“å°ç¬¬ä¸€ä¸ªç»“æœä½œä¸ºç¤ºä¾‹
            if results:
                print(f"\nExample output for '{results[0].get('name')}':")
                print(json.dumps(results[0], indent=2, ensure_ascii=False))
        else:
            print(f"Snapshot directory not found: {snapshot_dir}")
